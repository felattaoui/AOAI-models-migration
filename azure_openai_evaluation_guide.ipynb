{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure OpenAI Migration - Evaluation Guide\n",
    "\n",
    "This notebook demonstrates how to **evaluate your model migration** using two complementary approaches:\n",
    "\n",
    "| Approach | When to use | Speed | Where results live |\n",
    "|----------|-------------|-------|-------------------|\n",
    "| **Local (SDK)** | Quick prototyping, small test sets | Fast (seconds) | In this notebook |\n",
    "| **Cloud (Foundry)** | Full A/B testing, CI/CD, final decision | ~1 min | Azure AI Foundry portal |\n",
    "\n",
    "**Per scenario, we run:**\n",
    "1. **Quick local eval** — SDK `azure-ai-evaluation` on 2-3 test cases for fast feedback\n",
    "2. **Full cloud eval** — Foundry Evals API on ALL test cases for the official comparison\n",
    "\n",
    "> **Microsoft recommendation**: *\"Run local evaluations on small test data to assess prototypes, then move into predeployment testing to run evaluations on a large dataset.\"*\n",
    "\n",
    "## Pre-built scenarios\n",
    "\n",
    "| Scenario | Use Case | Key Metrics |\n",
    "|----------|----------|-------------|\n",
    "| **RAG** | Q&A over documents | Groundedness, Relevance, Coherence |\n",
    "| **Tool Calling** | Function/tool selection | Tool Accuracy, Parameter Accuracy |\n",
    "| **Translation** | Multilingual translation | Fluency, Coherence, Relevance |\n",
    "| **Classification** | Sentiment / categorization | Accuracy, Consistency |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# %pip install openai>=1.40.0 azure-identity>=1.15.0 python-dotenv>=1.0.0 -q\n",
    "# %pip install azure-ai-evaluation>=1.0.0 azure-ai-projects>=2.0.0b3 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Add project root to path so we can import src/\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "if os.path.basename(os.getcwd()) != 'AOAI-models-migration':\n",
    "    sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Suppress verbose SDK logs (promptflow execution.bulk, Run Summary, etc.)\n",
    "# Must be set BEFORE importing azure.ai.evaluation so child processes inherit it.\n",
    "os.environ[\"PF_LOGGING_LEVEL\"] = \"CRITICAL\"\n",
    "for _name in (\"azure.ai.evaluation\", \"promptflow\", \"execution\", \"execution.bulk\",\n",
    "              \"azure.core.pipeline.policies.http_logging_policy\"):\n",
    "    logging.getLogger(_name).setLevel(logging.CRITICAL)\n",
    "\n",
    "# Verify configuration\n",
    "from src.config import load_config, MODEL_REGISTRY, MIGRATION_PATHS\n",
    "config = load_config()\n",
    "display(Markdown(f\"**Endpoint:** `{config['endpoint'][:50]}...`\"))\n",
    "display(Markdown(f\"**Deployments:** `{list(config['deployments'].keys())}`\"))\n",
    "\n",
    "# SDK model config for local evaluation (same evaluators as Foundry cloud)\n",
    "from src.evaluate.local_eval import get_model_config, compare_local\n",
    "model_config = get_model_config()\n",
    "display(Markdown(f\"**Local eval model:** `{model_config['azure_deployment']}`\"))\n",
    "\n",
    "# Foundry client for cloud evaluation\n",
    "from src.evaluate.foundry import FoundryEvalsClient, FOUNDRY_AVAILABLE\n",
    "foundry = None\n",
    "if FOUNDRY_AVAILABLE and os.getenv(\"AZURE_AI_PROJECT_ENDPOINT\"):\n",
    "    foundry = FoundryEvalsClient()\n",
    "    display(Markdown(f\"**Foundry endpoint:** `{os.getenv('AZURE_AI_PROJECT_ENDPOINT')[:60]}...`\"))\n",
    "else:\n",
    "    display(Markdown(\"**Foundry** not configured — cloud evaluation cells will be skipped\"))\n",
    "\n",
    "\n",
    "def foundry_compare(source_items, target_items, metrics,\n",
    "                    scenario=\"eval\",\n",
    "                    source_label=\"gpt-4o\", target_label=\"gpt-4.1\"):\n",
    "    \"\"\"Run Foundry cloud eval on source vs target items, display summary.\n",
    "\n",
    "    Both runs share the same timestamp so they can be identified as a pair\n",
    "    in the Foundry portal. Run names follow the pattern:\n",
    "        {scenario}_{model}_{timestamp}\n",
    "    Example: rag_gpt-4o_20260217_1037 / rag_gpt-4.1_20260217_1037\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "\n",
    "    # Shared timestamp to link the pair of runs\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    pair_id = f\"{scenario}_{source_label}_vs_{target_label}_{ts}\"\n",
    "\n",
    "    display(Markdown(f\"**Comparison pair:** `{pair_id}`\"))\n",
    "\n",
    "    src_name = f\"{scenario}_{source_label}_{ts}\"\n",
    "    tgt_name = f\"{scenario}_{target_label}_{ts}\"\n",
    "\n",
    "    display(Markdown(f\"#### Source: {source_label} ({len(source_items)} items) — `{src_name}`\"))\n",
    "    source_result = foundry.evaluate_items(\n",
    "        source_items, metrics=metrics, eval_name=src_name,\n",
    "    )\n",
    "\n",
    "    display(Markdown(f\"#### Target: {target_label} ({len(target_items)} items) — `{tgt_name}`\"))\n",
    "    target_result = foundry.evaluate_items(\n",
    "        target_items, metrics=metrics, eval_name=tgt_name,\n",
    "    )\n",
    "\n",
    "    # Summary as markdown table\n",
    "    lines = [\n",
    "        f\"### Foundry Comparison: {source_label} vs {target_label} (`{scenario}`)\\n\",\n",
    "        f\"| Metric | {source_label} | {target_label} | Delta |\",\n",
    "        \"|--------|-------:|-------:|------:|\",\n",
    "    ]\n",
    "    for metric in metrics:\n",
    "        src = [s[metric][\"score\"] for s in source_result[\"scores\"] if s.get(metric, {}).get(\"score\") is not None]\n",
    "        tgt = [s[metric][\"score\"] for s in target_result[\"scores\"] if s.get(metric, {}).get(\"score\") is not None]\n",
    "        src_avg = sum(src) / max(len(src), 1)\n",
    "        tgt_avg = sum(tgt) / max(len(tgt), 1)\n",
    "        delta = tgt_avg - src_avg\n",
    "        lines.append(f\"| {metric} | {src_avg:.2f} | {tgt_avg:.2f} | {delta:+.2f} |\")\n",
    "\n",
    "    portal_src = source_result.get('report_url', 'N/A')\n",
    "    portal_tgt = target_result.get('report_url', 'N/A')\n",
    "    lines.append(f\"\\n[Source portal]({portal_src}) | [Target portal]({portal_tgt})\")\n",
    "    display(Markdown(\"\\n\".join(lines)))\n",
    "\n",
    "    return {\"source\": source_result, \"target\": target_result, \"pair_id\": pair_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scenario 1: RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "Tests whether the new model generates responses that are **grounded in the provided context**.\n",
    "\n",
    "**Key question**: Does the new model hallucinate more or less than the old one?\n",
    "\n",
    "### Pre-built examples include:\n",
    "- Company policy documents (remote work, data retention)\n",
    "- Technical documentation (API configuration, webhooks)\n",
    "- Financial reports\n",
    "- Questions NOT in context (tests hallucination resistance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate.scenarios.rag import RAG_TEST_CASES, create_rag_evaluator\n",
    "\n",
    "# Preview the test data\n",
    "lines = [f\"**RAG test cases: {len(RAG_TEST_CASES)}**\\n\"]\n",
    "for i, tc in enumerate(RAG_TEST_CASES[:3]):\n",
    "    lines.append(f\"---\\n**Test {i+1}**\")\n",
    "    lines.append(f\"- **Question:** {tc.prompt}\")\n",
    "    lines.append(f\"- **Context:** `{tc.context[:100]}...`\")\n",
    "    lines.append(f\"- **Expected:** `{tc.expected_output[:100]}...`\\n\")\n",
    "display(Markdown(\"\\n\".join(lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Collect responses from both models (all test cases)\n",
    "from src.evaluate.scenarios.rag import RAG_TEST_CASES, create_rag_evaluator\n",
    "\n",
    "rag_evaluator = create_rag_evaluator(\n",
    "    source_model=\"gpt-4o\",\n",
    "    target_model=\"gpt-4.1\",\n",
    "    source_deployment=os.getenv(\"GPT4O_DEPLOYMENT\"),\n",
    "    target_deployment=os.getenv(\"GPT41_DEPLOYMENT\"),\n",
    ")\n",
    "\n",
    "rag_source_all, rag_target_all = rag_evaluator.collect()\n",
    "\n",
    "# Step 2: Quick local eval on first 3 test cases (SDK azure-ai-evaluation)\n",
    "display(Markdown(f\"### Quick local eval (3 / {len(RAG_TEST_CASES)} test cases)\"))\n",
    "rag_local = compare_local(\n",
    "    rag_source_all[:3], rag_target_all[:3],\n",
    "    metrics=[\"coherence\", \"fluency\", \"relevance\", \"groundedness\"],\n",
    "    model_config=model_config,\n",
    "    source_label=\"gpt-4o\",\n",
    "    target_label=\"gpt-4.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full cloud eval — RAG (all 8 test cases scored in Foundry)\n",
    "if foundry:\n",
    "    rag_cloud = foundry_compare(\n",
    "        rag_source_all, rag_target_all,\n",
    "        metrics=[\"coherence\", \"fluency\", \"relevance\", \"groundedness\"],\n",
    "        scenario=\"rag\",\n",
    "        source_label=\"gpt-4o\", target_label=\"gpt-4.1\",\n",
    "    )\n",
    "else:\n",
    "    display(Markdown(\"**Foundry not configured** — skipping cloud evaluation.  \\nSet `AZURE_AI_PROJECT_ENDPOINT` in `.env` and install `azure-ai-projects`.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scenario 2: Tool Calling\n",
    "\n",
    "Tests whether the new model **selects the correct tools** and **extracts parameters accurately**.\n",
    "\n",
    "### Pre-built examples include:\n",
    "- Weather queries (simple tool selection)\n",
    "- Product search (multiple parameter extraction)\n",
    "- Calendar events (complex parameters with attendees, location)\n",
    "- Email composition (content generation + tool call)\n",
    "- Stock inventory check (product and warehouse extraction)\n",
    "- Ambiguous requests (tests tool selection logic)\n",
    "- No-tool-needed queries (should respond directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate.scenarios.tool_calling import (\n",
    "    TOOL_CALLING_TEST_CASES, SAMPLE_TOOLS, create_tool_calling_evaluator\n",
    ")\n",
    "\n",
    "# Preview available tools\n",
    "lines = [\"**Available tools:**\\n\"]\n",
    "for tool in SAMPLE_TOOLS:\n",
    "    name = tool[\"function\"][\"name\"]\n",
    "    desc = tool[\"function\"][\"description\"]\n",
    "    lines.append(f\"- `{name}`: {desc}\")\n",
    "\n",
    "lines.append(f\"\\n**Test cases: {len(TOOL_CALLING_TEST_CASES)}**\\n\")\n",
    "for i, tc in enumerate(TOOL_CALLING_TEST_CASES[:3]):\n",
    "    lines.append(f\"{i+1}. {tc.prompt[:60]}... — expected: `{tc.metadata['expected_tool']}`\")\n",
    "display(Markdown(\"\\n\".join(lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Collect responses from both models (all test cases, with tools)\n",
    "from src.evaluate.scenarios.tool_calling import (\n",
    "    TOOL_CALLING_TEST_CASES, create_tool_calling_evaluator\n",
    ")\n",
    "\n",
    "tc_evaluator = create_tool_calling_evaluator(\n",
    "    source_model=\"gpt-4o\",\n",
    "    target_model=\"gpt-4.1\",\n",
    "    source_deployment=os.getenv(\"GPT4O_DEPLOYMENT\"),\n",
    "    target_deployment=os.getenv(\"GPT41_DEPLOYMENT\"),\n",
    ")\n",
    "\n",
    "tc_source_all, tc_target_all = tc_evaluator.collect()\n",
    "\n",
    "# Step 2: Show deterministic scores (tool_accuracy, param_accuracy) on first 3\n",
    "lines = [\n",
    "    f\"### Deterministic scores (3 / {len(TOOL_CALLING_TEST_CASES)} test cases)\\n\",\n",
    "    \"| Test | Metric | Source | Target |\",\n",
    "    \"|------|--------|-------:|-------:|\",\n",
    "]\n",
    "for i in range(min(3, len(tc_source_all))):\n",
    "    desc = TOOL_CALLING_TEST_CASES[i].metadata.get(\"description\", \"\")[:45]\n",
    "    for metric in [\"tool_accuracy\", \"param_accuracy\"]:\n",
    "        s = tc_source_all[i][\"_scores\"].get(metric, 0)\n",
    "        t = tc_target_all[i][\"_scores\"].get(metric, 0)\n",
    "        lines.append(f\"| {desc} | {metric} | {s:.1f} | {t:.1f} |\")\n",
    "        desc = \"\"\n",
    "display(Markdown(\"\\n\".join(lines)))\n",
    "\n",
    "# Step 3: Quick local eval with SDK (text quality on first 3)\n",
    "display(Markdown(\"### SDK local eval (text quality)\"))\n",
    "tc_local = compare_local(\n",
    "    tc_source_all[:3], tc_target_all[:3],\n",
    "    metrics=[\"coherence\", \"relevance\"],\n",
    "    model_config=model_config,\n",
    "    source_label=\"gpt-4o\",\n",
    "    target_label=\"gpt-4.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full cloud eval — Tool Calling (all 8 test cases scored in Foundry)\n",
    "if foundry:\n",
    "    tc_cloud = foundry_compare(\n",
    "        tc_source_all, tc_target_all,\n",
    "        metrics=[\"coherence\", \"relevance\", \"tool_call_accuracy\"],\n",
    "        scenario=\"tool_calling\",\n",
    "        source_label=\"gpt-4o\", target_label=\"gpt-4.1\",\n",
    "    )\n",
    "else:\n",
    "    display(Markdown(\"**Foundry not configured** — skipping cloud evaluation.  \\nSet `AZURE_AI_PROJECT_ENDPOINT` in `.env` and install `azure-ai-projects`.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scenario 3: Translation\n",
    "\n",
    "Tests whether the new model maintains **translation quality** across languages.\n",
    "\n",
    "### Pre-built examples include:\n",
    "- FR→EN: Business, legal, idiomatic expressions, cultural references\n",
    "- EN→FR: Technical docs, marketing, UI text, medical\n",
    "- EN→DE: Informal/conversational\n",
    "- Tricky cases: homographs, ambiguous sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate.scenarios.translation import TRANSLATION_TEST_CASES, create_translation_evaluator\n",
    "\n",
    "# Preview test cases\n",
    "lines = [f\"**Translation test cases: {len(TRANSLATION_TEST_CASES)}**\\n\",\n",
    "         \"| # | Direction | Domain | Difficulty | Source |\",\n",
    "         \"|---|-----------|--------|------------|--------|\"]\n",
    "for i, tc in enumerate(TRANSLATION_TEST_CASES):\n",
    "    meta = tc.metadata\n",
    "    lines.append(f\"| {i+1} | {meta.get('direction', '?')} | {meta.get('domain', '?')} | {meta.get('difficulty', '?')} | {tc.prompt[:50]}... |\")\n",
    "display(Markdown(\"\\n\".join(lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Collect responses from both models (all test cases)\n",
    "from src.evaluate.scenarios.translation import TRANSLATION_TEST_CASES, create_translation_evaluator\n",
    "\n",
    "trans_evaluator = create_translation_evaluator(\n",
    "    source_model=\"gpt-4o\",\n",
    "    target_model=\"gpt-4.1\",\n",
    "    source_deployment=os.getenv(\"GPT4O_DEPLOYMENT\"),\n",
    "    target_deployment=os.getenv(\"GPT41_DEPLOYMENT\"),\n",
    ")\n",
    "\n",
    "trans_source_all, trans_target_all = trans_evaluator.collect()\n",
    "\n",
    "# Step 2: Quick local eval on first 3 test cases (SDK azure-ai-evaluation)\n",
    "display(Markdown(f\"### Quick local eval (3 / {len(TRANSLATION_TEST_CASES)} test cases)\"))\n",
    "trans_local = compare_local(\n",
    "    trans_source_all[:3], trans_target_all[:3],\n",
    "    metrics=[\"coherence\", \"fluency\", \"relevance\"],\n",
    "    model_config=model_config,\n",
    "    source_label=\"gpt-4o\",\n",
    "    target_label=\"gpt-4.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full cloud eval — Translation (all 10 test cases scored in Foundry)\n",
    "if foundry:\n",
    "    trans_cloud = foundry_compare(\n",
    "        trans_source_all, trans_target_all,\n",
    "        metrics=[\"coherence\", \"fluency\", \"relevance\"],\n",
    "        scenario=\"translation\",\n",
    "        source_label=\"gpt-4o\", target_label=\"gpt-4.1\",\n",
    "    )\n",
    "else:\n",
    "    display(Markdown(\"**Foundry not configured** — skipping cloud evaluation.  \\nSet `AZURE_AI_PROJECT_ENDPOINT` in `.env` and install `azure-ai-projects`.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scenario 4: Classification / Sentiment Analysis\n",
    "\n",
    "Tests whether the new model maintains **classification accuracy and consistency**.\n",
    "\n",
    "### Pre-built examples include:\n",
    "- Sentiment analysis (positive/negative/neutral, sarcasm detection)\n",
    "- Support ticket classification (billing, technical, account, shipping)\n",
    "- Intent classification (flight booking, cancellation, baggage)\n",
    "- IT incident priority (P1-P4)\n",
    "- Edge cases: mixed sentiment, ambiguous tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate.scenarios.classification import (\n",
    "    CLASSIFICATION_TEST_CASES, create_classification_evaluator\n",
    ")\n",
    "\n",
    "# Preview test cases by task type\n",
    "from collections import Counter\n",
    "tasks = Counter(tc.metadata.get('task', '?') for tc in CLASSIFICATION_TEST_CASES)\n",
    "\n",
    "lines = [f\"**Classification test cases: {len(CLASSIFICATION_TEST_CASES)}**\",\n",
    "         f\"By task: `{dict(tasks)}`\\n\",\n",
    "         \"| # | Task | Expected | Prompt |\",\n",
    "         \"|---|------|----------|--------|\"]\n",
    "for i, tc in enumerate(CLASSIFICATION_TEST_CASES[:5]):\n",
    "    lines.append(f\"| {i+1} | {tc.metadata.get('task')} | `{tc.ground_truth_label}` | {tc.prompt[:60]}... |\")\n",
    "display(Markdown(\"\\n\".join(lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Collect responses from both models (all test cases)\n",
    "from src.evaluate.scenarios.classification import (\n",
    "    CLASSIFICATION_TEST_CASES, create_classification_evaluator\n",
    ")\n",
    "\n",
    "cls_evaluator = create_classification_evaluator(\n",
    "    source_model=\"gpt-4o\",\n",
    "    target_model=\"gpt-4.1\",\n",
    "    source_deployment=os.getenv(\"GPT4O_DEPLOYMENT\"),\n",
    "    target_deployment=os.getenv(\"GPT41_DEPLOYMENT\"),\n",
    "    consistency_runs=3,\n",
    ")\n",
    "\n",
    "cls_source_all, cls_target_all = cls_evaluator.collect()\n",
    "\n",
    "# Step 2: Deterministic accuracy on first 3 test cases\n",
    "lines = [\n",
    "    f\"### Classification accuracy (3 / {len(CLASSIFICATION_TEST_CASES)} test cases)\\n\",\n",
    "    \"| Test | Expected | Source | Target |\",\n",
    "    \"|------|----------|--------|--------|\",\n",
    "]\n",
    "for i in range(min(3, len(cls_source_all))):\n",
    "    prompt = CLASSIFICATION_TEST_CASES[i].prompt[:45]\n",
    "    expected = CLASSIFICATION_TEST_CASES[i].ground_truth_label or \"?\"\n",
    "    src_pred = cls_source_all[i][\"_prediction\"]\n",
    "    tgt_pred = cls_target_all[i][\"_prediction\"]\n",
    "    src_ok = \"ok\" if cls_source_all[i][\"_accuracy\"] >= 4.0 else \"MISS\"\n",
    "    tgt_ok = \"ok\" if cls_target_all[i][\"_accuracy\"] >= 4.0 else \"MISS\"\n",
    "    lines.append(f\"| {prompt}... | `{expected}` | {src_ok} `{src_pred}` | {tgt_ok} `{tgt_pred}` |\")\n",
    "display(Markdown(\"\\n\".join(lines)))\n",
    "\n",
    "# Step 3: F1 score via SDK (token-level match against ground truth)\n",
    "display(Markdown(\"### SDK local eval (F1 score)\"))\n",
    "cls_local = compare_local(\n",
    "    cls_source_all[:3], cls_target_all[:3],\n",
    "    metrics=[\"f1_score\"],\n",
    "    model_config=model_config,\n",
    "    source_label=\"gpt-4o\",\n",
    "    target_label=\"gpt-4.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full cloud eval — Classification (all test cases)\n",
    "# Note: Foundry doesn't have a built-in classification accuracy metric.\n",
    "# We use deterministic accuracy (above) + relevance for cloud comparison.\n",
    "if foundry:\n",
    "    cls_cloud = foundry_compare(\n",
    "        cls_source_all, cls_target_all,\n",
    "        metrics=[\"relevance\"],\n",
    "        scenario=\"classification\",\n",
    "        source_label=\"gpt-4o\", target_label=\"gpt-4.1\",\n",
    "    )\n",
    "\n",
    "    # Full deterministic accuracy table (all test cases)\n",
    "    lines = [\n",
    "        f\"### Full deterministic accuracy ({len(cls_source_all)} test cases)\\n\",\n",
    "        \"| # | Task | Expected | Source | Target |\",\n",
    "        \"|---|------|----------|--------|--------|\",\n",
    "    ]\n",
    "    src_correct = tgt_correct = 0\n",
    "    for i in range(len(cls_source_all)):\n",
    "        task = CLASSIFICATION_TEST_CASES[i].metadata.get(\"task\", \"?\")\n",
    "        expected = CLASSIFICATION_TEST_CASES[i].ground_truth_label or \"?\"\n",
    "        src_pred = cls_source_all[i][\"_prediction\"]\n",
    "        tgt_pred = cls_target_all[i][\"_prediction\"]\n",
    "        src_ok = cls_source_all[i][\"_accuracy\"] >= 4.0\n",
    "        tgt_ok = cls_target_all[i][\"_accuracy\"] >= 4.0\n",
    "        src_correct += src_ok\n",
    "        tgt_correct += tgt_ok\n",
    "        src_mark = \"ok\" if src_ok else \"**MISS**\"\n",
    "        tgt_mark = \"ok\" if tgt_ok else \"**MISS**\"\n",
    "        lines.append(f\"| {i+1} | {task} | `{expected}` | {src_mark} `{src_pred}` | {tgt_mark} `{tgt_pred}` |\")\n",
    "\n",
    "    n = len(cls_source_all)\n",
    "    lines.append(f\"\\n**Accuracy:** source={src_correct}/{n} ({100*src_correct/n:.0f}%) — target={tgt_correct}/{n} ({100*tgt_correct/n:.0f}%)\")\n",
    "    display(Markdown(\"\\n\".join(lines)))\n",
    "else:\n",
    "    display(Markdown(\"**Foundry not configured** — skipping cloud evaluation.  \\nSet `AZURE_AI_PROJECT_ENDPOINT` in `.env` and install `azure-ai-projects`.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Decision Guide\n",
    "\n",
    "### Local vs Cloud evaluation\n",
    "\n",
    "| | Local (SDK) | Cloud (Foundry) |\n",
    "|---|---|---|\n",
    "| **Speed** | Seconds | ~1 minute per model |\n",
    "| **Results** | In notebook only | In Foundry portal, shareable |\n",
    "| **Best for** | Quick iteration, prototyping | Final decision, CI/CD gates |\n",
    "| **Metrics** | Same algorithms as Foundry | Authoritative scores |\n",
    "| **Custom metrics** | Deterministic only (tool accuracy, etc.) | Built-in evaluators only |\n",
    "\n",
    "### When is migration safe?\n",
    "\n",
    "| Result | Action |\n",
    "|--------|--------|\n",
    "| **0 regressions** | Safe to migrate. Proceed with confidence. |\n",
    "| **< 10% regressions** | Caution. Review flagged cases. Consider prompt tuning. |\n",
    "| **> 10% regressions** | Risky. Investigate root causes before proceeding. |\n",
    "\n",
    "### Recommended regression thresholds\n",
    "\n",
    "| Metric | Acceptable Delta | Action if exceeded |\n",
    "|--------|-----------------|-------------------|\n",
    "| Coherence | -0.5 | Review response structure |\n",
    "| Fluency | -0.5 | Check language quality |\n",
    "| Relevance | -1.0 | Review prompt/system message |\n",
    "| Groundedness | -0.5 | Critical for RAG — investigate hallucinations |\n",
    "| Tool Accuracy | Any drop | Critical — wrong tool = wrong action |\n",
    "| Classification Accuracy | Any drop | Review system prompt clarity |\n",
    "\n",
    "### How to adapt with your own data\n",
    "\n",
    "1. **Pick the closest scenario** from the 4 above\n",
    "2. **Replace test data** with your production prompts:\n",
    "\n",
    "```python\n",
    "from src.evaluate.core import TestCase\n",
    "\n",
    "my_tests = [\n",
    "    TestCase(\n",
    "        prompt=\"Your question here\",\n",
    "        context=\"Your document context...\",\n",
    "        expected_output=\"Expected answer\",\n",
    "    ),\n",
    "]\n",
    "```\n",
    "\n",
    "3. **Create evaluator** with your test cases: `create_rag_evaluator(\"gpt-4o\", \"gpt-4.1\", test_cases=my_tests)`\n",
    "4. **Run the same flow**: `collect()` → `compare_local()` → `foundry_compare()`\n",
    "5. **Iterate** until regressions are within acceptable thresholds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
