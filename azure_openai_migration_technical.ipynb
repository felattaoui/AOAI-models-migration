{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Technical Migration Guide: GPT-4o & GPT-4o-mini to GPT-5.1 & GPT-4.1-mini\n\nThis notebook provides a **code-focused** guide for migrating from GPT-4o and GPT-4o-mini to their recommended successors on Azure OpenAI.\n\n## Migration Paths\n\n| Source Model | Target Model | Model Type |\n|--------------|--------------|------------|\n| **GPT-4o** (all versions) | **GPT-5.1** | Reasoning model |\n| **GPT-4o-mini** | **GPT-4.1-mini** | Standard model |\n\n> **Important**: GPT-5.1 is a reasoning model (no temperature/top_p). GPT-4.1-mini is NOT a reasoning model and supports standard parameters.\n\n## Related Resources\n\n- **Microsoft Learn**: [Azure OpenAI Reasoning Models](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/reasoning)\n- **Model Retirements**: [Model Retirement Dates](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/model-retirements)\n\n---\n\n## Migration Strategy\n\n1. **Phase 1: Lift & Shift** - Minimal code changes to get the new models running\n2. **Phase 2: Optimization** - Leverage GPT-5.1 specific features (reasoning models only)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 1: Lift & Shift\n",
    "\n",
    "The goal of Phase 1 is to migrate with **minimal changes** to establish a baseline and validate that the new model works correctly with your existing prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install Dependencies\n",
    "\n",
    "Install the required packages using the provided `requirements.txt`:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Per [Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/reasoning):\n",
    "\n",
    "> *\"You'll need to upgrade your OpenAI client library for access to the latest parameters.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required packages:\n",
      "  - openai>=1.40.0\n",
      "  - azure-identity>=1.15.0\n",
      "  - python-dotenv>=1.0.0\n",
      "\n",
      "Install with: pip install -r requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies from requirements.txt\n",
    "# Run this in your terminal: pip install -r requirements.txt\n",
    "\n",
    "# Or uncomment to install directly from notebook:\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "print(\"Required packages:\")\n",
    "print(\"  - openai>=1.40.0\")\n",
    "print(\"  - azure-identity>=1.15.0\")\n",
    "print(\"  - python-dotenv>=1.0.0\")\n",
    "print(\"\\nInstall with: pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configuration Changes\n",
    "\n",
    "### Key Differences: GPT-4o vs GPT-5.1 (and GPT-4o-mini vs GPT-4.1-mini)\n",
    "\n",
    "| Aspect | GPT-4o / GPT-4o-mini | GPT-5.1 | GPT-4.1-mini | Reference |\n",
    "|--------|---------------------|---------|--------------|----------|\n",
    "| **Endpoint** | `/openai/deployments/.../chat/completions?api-version=...` | `/openai/v1/chat/completions` | `/openai/v1/chat/completions` | [MS Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/reasoning) |\n",
    "| **API Version** | Required (e.g., `2024-02-15-preview`) | Not required (v1 API) | Not required (v1 API) | [MS Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/model-inference-to-openai-migration) |\n",
    "| **Max Tokens** | `max_tokens` | `max_completion_tokens` | `max_completion_tokens` | [MS Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/reasoning) |\n",
    "| **Temperature** | Supported | Not supported | Supported | [MS Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/reasoning) |\n",
    "| **Reasoning** | N/A | `reasoning_effort` | N/A | [OpenAI Docs](https://platform.openai.com/docs/guides/latest-model) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Endpoint: https://models-llm-migration-swe-eaf.cognitiveservices.azure.com\n",
      "  API Key:  ***wNdv\n",
      "\n",
      "Source Models:\n",
      "  GPT-4o:      gpt-4o\n",
      "  GPT-4o-mini: gpt-4o-mini\n",
      "\n",
      "Target Models:\n",
      "  GPT-5.1:      gpt-5.1\n",
      "  GPT-4.1-mini: gpt-4.1-mini\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION - Update these values for your Azure environment\n",
    "# =============================================================================\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"https://YOUR-RESOURCE.openai.azure.com\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "# =============================================================================\n",
    "# SOURCE MODEL DEPLOYMENTS (for comparison testing)\n",
    "# =============================================================================\n",
    "GPT4O_DEPLOYMENT = os.getenv(\"GPT4O_DEPLOYMENT\", \"gpt-4o\")\n",
    "GPT4O_MINI_DEPLOYMENT = os.getenv(\"GPT4O_MINI_DEPLOYMENT\", \"gpt-4o-mini\")\n",
    "\n",
    "# =============================================================================\n",
    "# TARGET MODEL DEPLOYMENTS (migration destinations)\n",
    "# =============================================================================\n",
    "GPT51_DEPLOYMENT = os.getenv(\"GPT51_DEPLOYMENT\", \"gpt-51\")           # For GPT-4o migration\n",
    "GPT41_MINI_DEPLOYMENT = os.getenv(\"GPT41_MINI_DEPLOYMENT\", \"gpt-41-mini\")  # For GPT-4o-mini migration\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "print(f\"  API Key:  {'***' + AZURE_OPENAI_API_KEY[-4:] if AZURE_OPENAI_API_KEY else 'NOT SET'}\")\n",
    "print(f\"\\nSource Models:\")\n",
    "print(f\"  GPT-4o:      {GPT4O_DEPLOYMENT}\")\n",
    "print(f\"  GPT-4o-mini: {GPT4O_MINI_DEPLOYMENT}\")\n",
    "print(f\"\\nTarget Models:\")\n",
    "print(f\"  GPT-5.1:      {GPT51_DEPLOYMENT}\")\n",
    "print(f\"  GPT-4.1-mini: {GPT41_MINI_DEPLOYMENT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Client Initialization: Before & After\n",
    "\n",
    "According to [Microsoft Learn - Migration Guide](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/model-inference-to-openai-migration):\n",
    "\n",
    "> *\"Change endpoint URLs from `.services.ai.azure.com/models` to `.openai.azure.com/openai/v1/`\"*\n",
    "\n",
    "> *\"The v1 API eliminates the need to frequently update `api-version` parameters.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o client initialized (Entra ID authentication)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BEFORE: GPT-4o Client Initialization (with Entra ID - recommended)\n",
    "# =============================================================================\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# Use Entra ID authentication (requires: az login)\n",
    "token_provider = get_bearer_token_provider(\n",
    "    DefaultAzureCredential(),\n",
    "    \"https://cognitiveservices.azure.com/.default\"\n",
    ")\n",
    "\n",
    "# Old approach with AzureOpenAI client and api_version\n",
    "client_gpt4o = AzureOpenAI(\n",
    "    azure_ad_token_provider=token_provider,  # Entra ID instead of api_key\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    ")\n",
    "\n",
    "print(\"GPT-4o client initialized (Entra ID authentication)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New client initialized (works for both GPT-5.1 and GPT-4.1-mini)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# AFTER: GPT-5.1 and GPT-4.1-mini Client Initialization (with Entra ID)\n",
    "# =============================================================================\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# New approach with OpenAI client and v1 endpoint\n",
    "# Same client works for both GPT-5.1 and GPT-4.1-mini\n",
    "# Using Entra ID token from previous cell\n",
    "client_new = OpenAI(\n",
    "    api_key=token_provider(),  # Get token from Entra ID\n",
    "    base_url=f\"{AZURE_OPENAI_ENDPOINT}/openai/v1/\"  # v1 API - no version needed\n",
    ")\n",
    "\n",
    "print(\"New client initialized (works for both GPT-5.1 and GPT-4.1-mini)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Microsoft Entra ID Authentication (Recommended for Production)\n",
    "\n",
    "From [Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/reasoning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client initialized with Entra ID (works for GPT-5.1 and GPT-4.1-mini)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ALTERNATIVE: Microsoft Entra ID Authentication (Recommended for Production)\n",
    "# =============================================================================\n",
    "\n",
    "from openai import OpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# Get token provider for Cognitive Services\n",
    "token_provider = get_bearer_token_provider(\n",
    "    DefaultAzureCredential(),\n",
    "    \"https://cognitiveservices.azure.com/.default\"\n",
    ")\n",
    "\n",
    "# Initialize client with Entra ID (works for all models)\n",
    "client_entra = OpenAI(\n",
    "    base_url=f\"{AZURE_OPENAI_ENDPOINT}/openai/v1/\",\n",
    "    api_key=token_provider  # Token provider instead of API key\n",
    ")\n",
    "\n",
    "print(\"Client initialized with Entra ID (works for GPT-5.1 and GPT-4.1-mini)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 API Call Changes: Before & After\n",
    "\n",
    "### Critical Parameter Changes\n",
    "\n",
    "From [Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/reasoning):\n",
    "\n",
    "> *\"Reasoning models will only work with the `max_completion_tokens` parameter when using the Chat Completions API.\"*\n",
    "\n",
    "### Two Migration Paths\n",
    "\n",
    "1. **GPT-4o -> GPT-5.1**: Remove temperature/top_p, add `reasoning_effort=\"none\"`\n",
    "2. **GPT-4o-mini -> GPT-4.1-mini**: Keep temperature/top_p, just update endpoint and `max_tokens` -> `max_completion_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o function defined\n",
      "GPT-4o-mini function defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BEFORE: GPT-4o and GPT-4o-mini API Calls\n",
    "# =============================================================================\n",
    "\n",
    "def call_gpt4o(user_message: str, system_prompt: str = \"You are a helpful assistant.\"):\n",
    "    \"\"\"GPT-4o call with original parameters\"\"\"\n",
    "    response = client_gpt4o.chat.completions.create(\n",
    "        model=GPT4O_DEPLOYMENT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        max_tokens=4096,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def call_gpt4o_mini(user_message: str, system_prompt: str = \"You are a helpful assistant.\"):\n",
    "    \"\"\"GPT-4o-mini call with original parameters\"\"\"\n",
    "    response = client_gpt4o.chat.completions.create(\n",
    "        model=GPT4O_MINI_DEPLOYMENT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        max_tokens=4096,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"GPT-4o function defined\")\n",
    "print(\"GPT-4o-mini function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5.1 Phase 1 function defined (reasoning model - no temperature)\n",
      "GPT-4.1-mini Phase 1 function defined (supports temperature)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# AFTER: GPT-5.1 and GPT-4.1-mini API Calls (Phase 1 - Lift & Shift)\n",
    "# =============================================================================\n",
    "\n",
    "def call_gpt51_phase1(user_message: str, system_prompt: str = \"You are a helpful assistant.\"):\n",
    "    \"\"\"\n",
    "    GPT-5.1 call with minimal changes for Phase 1 migration (from GPT-4o).\n",
    "    \n",
    "    Key changes:\n",
    "    - max_tokens -> max_completion_tokens\n",
    "    - Added reasoning_effort=\"none\" to match GPT-4o behavior\n",
    "    - Removed temperature and top_p (not supported with reasoning models)\n",
    "    - Kept \"system\" role (backward compatible)\n",
    "    \"\"\"\n",
    "    response = client_new.chat.completions.create(\n",
    "        model=GPT51_DEPLOYMENT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        max_completion_tokens=4096,\n",
    "        reasoning_effort=\"none\"  # Critical: match GPT-4o behavior\n",
    "        # No temperature, top_p (not supported in reasoning models)\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def call_gpt41_mini_phase1(user_message: str, system_prompt: str = \"You are a helpful assistant.\"):\n",
    "    \"\"\"\n",
    "    GPT-4.1-mini call for Phase 1 migration (from GPT-4o-mini).\n",
    "    \n",
    "    Key changes:\n",
    "    - max_tokens -> max_completion_tokens\n",
    "    - temperature and top_p are SUPPORTED (not a reasoning model)\n",
    "    \"\"\"\n",
    "    response = client_new.chat.completions.create(\n",
    "        model=GPT41_MINI_DEPLOYMENT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        max_completion_tokens=4096,\n",
    "        temperature=0.7,  # Supported in GPT-4.1-mini\n",
    "        top_p=0.95        # Supported in GPT-4.1-mini\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"GPT-5.1 Phase 1 function defined (reasoning model - no temperature)\")\n",
    "print(\"GPT-4.1-mini Phase 1 function defined (supports temperature)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Parameters NOT Supported in GPT-5.1 (Reasoning Models Only)\n",
    "\n",
    "From [Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/reasoning):\n",
    "\n",
    "> *\"The following are currently unsupported with reasoning models: `temperature`, `top_p`, `presence_penalty`, `frequency_penalty`, `logprobs`, `top_logprobs`, `logit_bias`, `max_tokens`\"*\n",
    "\n",
    "### This applies ONLY to GPT-5.1 (reasoning model)\n",
    "\n",
    "**GPT-4.1-mini supports all standard parameters** (temperature, top_p, etc.) because it is NOT a reasoning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters NOT supported in GPT-5.1 (reasoning model):\n",
      "   - temperature\n",
      "   - top_p\n",
      "   - presence_penalty\n",
      "   - frequency_penalty\n",
      "   - logprobs\n",
      "   - top_logprobs\n",
      "   - logit_bias\n",
      "   - max_tokens\n",
      "\n",
      "Use 'max_completion_tokens' instead of 'max_tokens'\n",
      "\n",
      "============================================================\n",
      "GPT-4.1-mini SUPPORTS all these parameters (not a reasoning model)\n",
      "   - temperature\n",
      "   - top_p\n",
      "   - etc.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# UNSUPPORTED PARAMETERS - For GPT-5.1 (reasoning model) ONLY\n",
    "# =============================================================================\n",
    "\n",
    "UNSUPPORTED_IN_GPT51 = [\n",
    "    \"temperature\",\n",
    "    \"top_p\",\n",
    "    \"presence_penalty\",\n",
    "    \"frequency_penalty\",\n",
    "    \"logprobs\",\n",
    "    \"top_logprobs\",\n",
    "    \"logit_bias\",\n",
    "    \"max_tokens\",  # Use max_completion_tokens instead\n",
    "]\n",
    "\n",
    "print(\"Parameters NOT supported in GPT-5.1 (reasoning model):\")\n",
    "for param in UNSUPPORTED_IN_GPT51:\n",
    "    print(f\"   - {param}\")\n",
    "\n",
    "print(\"\\nUse 'max_completion_tokens' instead of 'max_tokens'\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GPT-4.1-mini SUPPORTS all these parameters (not a reasoning model)\")\n",
    "print(\"   - temperature\")\n",
    "print(\"   - top_p\")\n",
    "print(\"   - etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Test Phase 1 Migration\n",
    "\n",
    "Following the [OpenAI GPT-5 Prompting Guide](https://developers.openai.com/cookbook/examples/gpt-5/gpt-5_prompting_guide):\n",
    "\n",
    "> *\"Step 3: Run Evals for a baseline. After model + effort are aligned, run your eval suite. If results look good (often better at med/high), you're ready to ship.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST: Comparing Migration Paths\n",
      "======================================================================\n",
      "\n",
      "Prompt: Explain microservices architecture in 3 sentences.\n",
      "System: You are a technical architect. Be concise and precise.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Migration Path 1: GPT-4o -> GPT-5.1\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "--- GPT-4o Response ---\n",
      "Microservices architecture is a design approach where an application is built as a collection of small, independent services, each responsible for a specific business functionality. These services communicate via lightweight protocols, typically HTTP/REST or messaging, and can be developed, deployed, and scaled independently. This architecture promotes modularity, agility, and fault isolation, making systems easier to maintain and evolve.\n",
      "Microservices architecture is an approach where an application is built as a suite of small, independently deployable services, each responsible for a specific business capability. These services communicate over lightweight protocols (commonly HTTP/REST, gRPC, or messaging) and can be developed, scaled, and deployed independently by different teams. This improves agility, scalability, and fault isolation, but adds complexity in areas like distributed data management, observability, and operational overhead.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TEST: Compare Source vs Target Models with identical prompts\n",
    "# =============================================================================\n",
    "\n",
    "test_prompt = \"Explain microservices architecture in 3 sentences.\"\n",
    "system_prompt = \"You are a technical architect. Be concise and precise.\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST: Comparing Migration Paths\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nPrompt: {test_prompt}\")\n",
    "print(f\"System: {system_prompt}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Migration Path 1: GPT-4o -> GPT-5.1\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Migration Path 1: GPT-4o -> GPT-5.1\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Uncomment to run actual tests (requires valid Azure credentials)\n",
    "print(\"\\n--- GPT-4o Response ---\")\n",
    "response_4o = call_gpt4o(test_prompt, system_prompt)\n",
    "print(response_4o)\n",
    "\n",
    "# print(\"\\n--- GPT-5.1 Response (Phase 1) ---\")\n",
    "response_51 = call_gpt51_phase1(test_prompt, system_prompt)\n",
    "print(response_51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Migration Path 2: GPT-4o-mini -> GPT-4.1-mini\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "--- GPT-4o-mini Response ---\n",
      "Microservices architecture is an approach to software development where applications are structured as a collection of loosely coupled, independently deployable services. Each service is designed to perform a specific business function and can be developed, deployed, and scaled independently. This architecture enhances flexibility, allows for continuous delivery, and enables teams to use different technologies for different services.\n",
      "Microservices architecture is a design approach where an application is composed of small, independent services that communicate over well-defined APIs. Each service focuses on a specific business capability and can be developed, deployed, and scaled independently. This architecture enhances flexibility, scalability, and fault isolation compared to monolithic systems.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Migration Path 2: GPT-4o-mini -> GPT-4.1-mini\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Migration Path 2: GPT-4o-mini -> GPT-4.1-mini\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Uncomment to run actual tests\n",
    "print(\"\\n--- GPT-4o-mini Response ---\")\n",
    "response_4o_mini = call_gpt4o_mini(test_prompt, system_prompt)\n",
    "print(response_4o_mini)\n",
    "\n",
    "# print(\"\\n--- GPT-4.1-mini Response (Phase 1) ---\")\n",
    "response_41_mini = call_gpt41_mini_phase1(test_prompt, system_prompt)\n",
    "print(response_41_mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Phase 1 Migration Checklist\n",
    "\n",
    "Based on [Microsoft Learn Migration Checklist](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/model-inference-to-openai-migration):\n",
    "\n",
    "> *\"Use this checklist to ensure a smooth migration:*\n",
    "> 1. *Install the OpenAI SDK for your programming language*\n",
    "> 2. *Update authentication code (API key or Microsoft Entra ID)*\n",
    "> 3. *Change endpoint URLs*\n",
    "> 4. *Update client initialization code*\n",
    "> 5. *Always specify the model parameter with your deployment name*\n",
    "> 6. *Update request method calls*\n",
    "> 7. *Test all functionality thoroughly\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1 Migration Checklist:\n",
      "============================================================\n",
      "\n",
      "For BOTH migration paths:\n",
      "  [x] SDK upgraded to latest version\n",
      "  [x] python-dotenv installed\n",
      "  [x] Endpoint changed to /openai/v1/\n",
      "  [x] Client uses OpenAI instead of AzureOpenAI\n",
      "  [x] api_version parameter removed\n",
      "  [x] max_tokens renamed to max_completion_tokens\n",
      "  [x] Prompts kept IDENTICAL to source model\n",
      "  [ ] Baseline evaluation completed\n",
      "\n",
      "For GPT-4o -> GPT-5.1 migration:\n",
      "  [x] reasoning_effort='none' added\n",
      "  [x] temperature parameter REMOVED\n",
      "  [x] top_p parameter REMOVED\n",
      "\n",
      "For GPT-4o-mini -> GPT-4.1-mini migration:\n",
      "  [x] temperature parameter KEPT\n",
      "  [x] top_p parameter KEPT\n",
      "  [x] No reasoning_effort needed\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PHASE 1 CHECKLIST\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Phase 1 Migration Checklist:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nFor BOTH migration paths:\")\n",
    "checklist_common = {\n",
    "    \"SDK upgraded to latest version\": True,\n",
    "    \"python-dotenv installed\": True,\n",
    "    \"Endpoint changed to /openai/v1/\": True,\n",
    "    \"Client uses OpenAI instead of AzureOpenAI\": True,\n",
    "    \"api_version parameter removed\": True,\n",
    "    \"max_tokens renamed to max_completion_tokens\": True,\n",
    "    \"Prompts kept IDENTICAL to source model\": True,\n",
    "    \"Baseline evaluation completed\": False,  # TODO: Run your evals\n",
    "}\n",
    "\n",
    "for item, completed in checklist_common.items():\n",
    "    status = \"[x]\" if completed else \"[ ]\"\n",
    "    print(f\"  {status} {item}\")\n",
    "\n",
    "print(\"\\nFor GPT-4o -> GPT-5.1 migration:\")\n",
    "checklist_gpt51 = {\n",
    "    \"reasoning_effort='none' added\": True,\n",
    "    \"temperature parameter REMOVED\": True,\n",
    "    \"top_p parameter REMOVED\": True,\n",
    "}\n",
    "\n",
    "for item, completed in checklist_gpt51.items():\n",
    "    status = \"[x]\" if completed else \"[ ]\"\n",
    "    print(f\"  {status} {item}\")\n",
    "\n",
    "print(\"\\nFor GPT-4o-mini -> GPT-4.1-mini migration:\")\n",
    "checklist_mini = {\n",
    "    \"temperature parameter KEPT\": True,\n",
    "    \"top_p parameter KEPT\": True,\n",
    "    \"No reasoning_effort needed\": True,\n",
    "}\n",
    "\n",
    "for item, completed in checklist_mini.items():\n",
    "    status = \"[x]\" if completed else \"[ ]\"\n",
    "    print(f\"  {status} {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 2: Optimization (GPT-5.1 Only)\n",
    "\n",
    "Once Phase 1 is validated, we can leverage GPT-5.1 specific features to improve performance.\n",
    "\n",
    "> **Note**: Phase 2 optimizations apply primarily to **GPT-5.1** (reasoning model). GPT-4.1-mini uses standard chat patterns and does not require these changes.\n",
    "\n",
    "From the [OpenAI GPT-5 Prompting Guide](https://developers.openai.com/cookbook/examples/gpt-5/gpt-5_prompting_guide):\n",
    "\n",
    "> *\"GPT-5.x models are especially well-suited for production agents that prioritize reliability, evaluability, and consistent behavior. They perform strongly across coding, document analysis, finance, and multi-tool agentic scenarios.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Developer Messages vs System Messages\n",
    "\n",
    "According to [Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/reasoning):\n",
    "\n",
    "> *\"Functionally developer messages `\"role\": \"developer\"` are the same as system messages.\"*\n",
    "\n",
    "> *\"When you use a system message with o4-mini, o3, o3-mini, and o1 it will be treated as a developer message. You should not use both a developer message and a system message in the same API request.\"*\n",
    "\n",
    "### Important Rule: Never mix `system` and `developer` roles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5.1 Phase 2 function defined (with developer role)\n",
      "\n",
      "Note: GPT-4.1-mini continues to use 'system' role (standard chat model)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PHASE 2: Using developer role (GPT-5.1 only)\n",
    "# =============================================================================\n",
    "\n",
    "def call_gpt51_phase2(user_message: str, developer_prompt: str = \"You are a helpful assistant.\"):\n",
    "    \"\"\"\n",
    "    GPT-5.1 call with Phase 2 optimizations.\n",
    "    \n",
    "    Key changes from Phase 1:\n",
    "    - \"system\" role -> \"developer\" role (more explicit for reasoning models)\n",
    "    \"\"\"\n",
    "    response = client_new.chat.completions.create(\n",
    "        model=GPT51_DEPLOYMENT,\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": developer_prompt},  # Use developer role\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        max_completion_tokens=4096,\n",
    "        reasoning_effort=\"none\"\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"GPT-5.1 Phase 2 function defined (with developer role)\")\n",
    "print(\"\\nNote: GPT-4.1-mini continues to use 'system' role (standard chat model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTI-PATTERN: Never use both 'system' and 'developer' in the same request!\n",
      "\n",
      "Bad example:\n",
      "   {'role': 'system', 'content': 'You are a helpful assistant.'}\n",
      "   {'role': 'developer', 'content': 'Always respond in French.'}\n",
      "   {'role': 'user', 'content': 'Hello'}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANTI-PATTERN: Never mix system and developer roles!\n",
    "# =============================================================================\n",
    "\n",
    "# This will cause issues - DO NOT DO THIS:\n",
    "bad_messages_example = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},     # BAD\n",
    "    {\"role\": \"developer\", \"content\": \"Always respond in French.\"},     # CONFLICT!\n",
    "    {\"role\": \"user\", \"content\": \"Hello\"}\n",
    "]\n",
    "\n",
    "print(\"ANTI-PATTERN: Never use both 'system' and 'developer' in the same request!\")\n",
    "print(\"\\nBad example:\")\n",
    "for msg in bad_messages_example:\n",
    "    print(f\"   {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Prompt Optimization for GPT-5.1\n",
    "\n",
    "According to the [OpenAI GPT-5 Prompting Guide](https://developers.openai.com/cookbook/examples/gpt-5/gpt-5_prompting_guide):\n",
    "\n",
    "> *\"GPT-5.x models deliver:*\n",
    "> - *More deliberate scaffolding: Builds clearer plans and intermediate structure by default*\n",
    "> - *Generally lower verbosity: More concise and task-focused*\n",
    "> - *Stronger instruction adherence: Less drift from user intent*\n",
    "> - *Conservative grounding bias: Tends to favor correctness and explicit reasoning\"*\n",
    "\n",
    "### 2.2.1 Verbosity Control\n",
    "\n",
    "> *\"Give clear and concrete length constraints especially in enterprise and coding agents.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbosity Control Prompt Template:\n",
      "\n",
      "<output_verbosity_spec>\n",
      "- Default: 3-6 sentences or <=5 bullets for typical answers.\n",
      "- For simple \"yes/no + short explanation\" questions: <=2 sentences.\n",
      "- For complex multi-step or multi-file tasks: \n",
      "  - 1 short overview paragraph\n",
      "  - then <=5 bullets tagged: What changed, Where, Risks, Next steps, Open questions.\n",
      "- Avoid long narrative paragraphs; prefer compact bullets and short sections.\n",
      "- Do not rephrase the user's request unless it changes semantics.\n",
      "</output_verbosity_spec>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VERBOSITY CONTROL - Recommended prompt pattern\n",
    "# =============================================================================\n",
    "\n",
    "VERBOSITY_CONTROL_PROMPT = \"\"\"\n",
    "<output_verbosity_spec>\n",
    "- Default: 3-6 sentences or <=5 bullets for typical answers.\n",
    "- For simple \"yes/no + short explanation\" questions: <=2 sentences.\n",
    "- For complex multi-step or multi-file tasks: \n",
    "  - 1 short overview paragraph\n",
    "  - then <=5 bullets tagged: What changed, Where, Risks, Next steps, Open questions.\n",
    "- Avoid long narrative paragraphs; prefer compact bullets and short sections.\n",
    "- Do not rephrase the user's request unless it changes semantics.\n",
    "</output_verbosity_spec>\n",
    "\"\"\"\n",
    "\n",
    "print(\"Verbosity Control Prompt Template:\")\n",
    "print(VERBOSITY_CONTROL_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Scope Drift Prevention\n",
    "\n",
    "From the [OpenAI GPT-5 Prompting Guide](https://developers.openai.com/cookbook/examples/gpt-5/gpt-5_prompting_guide):\n",
    "\n",
    "> *\"GPT-5.x is stronger at structured code but may produce more code than the minimal UX specs and design systems. To stay within the scope, explicitly forbid extra features and uncontrolled styling.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scope Control Prompt Template:\n",
      "\n",
      "<design_and_scope_constraints>\n",
      "- Explore any existing design systems and understand it deeply. \n",
      "- Implement EXACTLY and ONLY what the user requests.\n",
      "- No extra features, no added components, no UX embellishments.\n",
      "- Style aligned to the design system at hand. \n",
      "- Do NOT invent colors, shadows, tokens, animations, or new UI elements, unless requested or necessary to the requirements. \n",
      "- If any instruction is ambiguous, choose the simplest valid interpretation.\n",
      "</design_and_scope_constraints>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SCOPE DRIFT PREVENTION - Recommended prompt pattern\n",
    "# =============================================================================\n",
    "\n",
    "SCOPE_CONTROL_PROMPT = \"\"\"\n",
    "<design_and_scope_constraints>\n",
    "- Explore any existing design systems and understand it deeply. \n",
    "- Implement EXACTLY and ONLY what the user requests.\n",
    "- No extra features, no added components, no UX embellishments.\n",
    "- Style aligned to the design system at hand. \n",
    "- Do NOT invent colors, shadows, tokens, animations, or new UI elements, unless requested or necessary to the requirements. \n",
    "- If any instruction is ambiguous, choose the simplest valid interpretation.\n",
    "</design_and_scope_constraints>\n",
    "\"\"\"\n",
    "\n",
    "print(\"Scope Control Prompt Template:\")\n",
    "print(SCOPE_CONTROL_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Ambiguity and Hallucination Prevention\n",
    "\n",
    "From the [OpenAI GPT-5 Prompting Guide](https://developers.openai.com/cookbook/examples/gpt-5/gpt-5_prompting_guide):\n",
    "\n",
    "> *\"Configure the prompt for overconfident hallucinations on ambiguous queries (e.g., unclear requirements, missing constraints, or questions that need fresh data but no tools are called).\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguity Handling Prompt Template:\n",
      "\n",
      "<uncertainty_and_ambiguity>\n",
      "- If the question is ambiguous or underspecified, explicitly call this out and:\n",
      "  - Ask up to 1-3 precise clarifying questions, OR\n",
      "  - Present 2-3 plausible interpretations with clearly labeled assumptions.\n",
      "- When external facts may have changed recently (prices, releases, policies) and no tools are available:\n",
      "  - Answer in general terms and state that details may have changed.\n",
      "- Never fabricate exact figures, line numbers, or external references when you are uncertain.\n",
      "- When you are unsure, prefer language like \"Based on the provided context...\" instead of absolute claims.\n",
      "</uncertainty_and_ambiguity>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# AMBIGUITY HANDLING - Recommended prompt pattern\n",
    "# =============================================================================\n",
    "\n",
    "AMBIGUITY_HANDLING_PROMPT = \"\"\"\n",
    "<uncertainty_and_ambiguity>\n",
    "- If the question is ambiguous or underspecified, explicitly call this out and:\n",
    "  - Ask up to 1-3 precise clarifying questions, OR\n",
    "  - Present 2-3 plausible interpretations with clearly labeled assumptions.\n",
    "- When external facts may have changed recently (prices, releases, policies) and no tools are available:\n",
    "  - Answer in general terms and state that details may have changed.\n",
    "- Never fabricate exact figures, line numbers, or external references when you are uncertain.\n",
    "- When you are unsure, prefer language like \"Based on the provided context...\" instead of absolute claims.\n",
    "</uncertainty_and_ambiguity>\n",
    "\"\"\"\n",
    "\n",
    "print(\"Ambiguity Handling Prompt Template:\")\n",
    "print(AMBIGUITY_HANDLING_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Complete Optimized Developer Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Developer Prompt for GPT-5.1:\n",
      "============================================================\n",
      "You are a senior software architect specializing in cloud solutions.\n",
      "\n",
      "<output_verbosity_spec>\n",
      "- Default: 3-6 sentences or <=5 bullets for typical answers.\n",
      "- For simple questions: <=2 sentences.\n",
      "- For complex tasks: 1 overview paragraph + <=5 tagged bullets.\n",
      "- Avoid long narrative paragraphs; prefer concise responses.\n",
      "</output_verbosity_spec>\n",
      "\n",
      "<scope_constraints>\n",
      "- Implement EXACTLY and ONLY what the user requests.\n",
      "- No extra features or embellishments unless explicitly requested.\n",
      "- If instructions are ambiguous, choose the simplest valid interpretation.\n",
      "</scope_constraints>\n",
      "\n",
      "<uncertainty_handling>\n",
      "- If uncertain, acknowledge it explicitly.\n",
      "- Never fabricate exact figures or references.\n",
      "- Use \"Based on the provided context...\" when unsure.\n",
      "</uncertainty_handling>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPLETE OPTIMIZED DEVELOPER PROMPT FOR GPT-5.1\n",
    "# =============================================================================\n",
    "\n",
    "def build_optimized_developer_prompt(base_instructions: str) -> str:\n",
    "    \"\"\"\n",
    "    Build an optimized developer prompt for GPT-5.1 by adding\n",
    "    recommended control patterns from the OpenAI guide.\n",
    "    \"\"\"\n",
    "    return f\"\"\"{base_instructions}\n",
    "\n",
    "<output_verbosity_spec>\n",
    "- Default: 3-6 sentences or <=5 bullets for typical answers.\n",
    "- For simple questions: <=2 sentences.\n",
    "- For complex tasks: 1 overview paragraph + <=5 tagged bullets.\n",
    "- Avoid long narrative paragraphs; prefer concise responses.\n",
    "</output_verbosity_spec>\n",
    "\n",
    "<scope_constraints>\n",
    "- Implement EXACTLY and ONLY what the user requests.\n",
    "- No extra features or embellishments unless explicitly requested.\n",
    "- If instructions are ambiguous, choose the simplest valid interpretation.\n",
    "</scope_constraints>\n",
    "\n",
    "<uncertainty_handling>\n",
    "- If uncertain, acknowledge it explicitly.\n",
    "- Never fabricate exact figures or references.\n",
    "- Use \"Based on the provided context...\" when unsure.\n",
    "</uncertainty_handling>\n",
    "\"\"\"\n",
    "\n",
    "# Example usage\n",
    "base_prompt = \"You are a senior software architect specializing in cloud solutions.\"\n",
    "optimized_prompt = build_optimized_developer_prompt(base_prompt)\n",
    "\n",
    "print(\"Optimized Developer Prompt for GPT-5.1:\")\n",
    "print(\"=\" * 60)\n",
    "print(optimized_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Using the Responses API (Advanced - GPT-5.1 Only)\n",
    "\n",
    "For more advanced control, GPT-5.1 supports the Responses API with additional parameters.\n",
    "\n",
    "From [Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/reasoning):\n",
    "\n",
    "> *\"When using the latest reasoning models with the Responses API you can use the reasoning summary parameter to receive summaries of the model's chain of thought reasoning.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses API function defined (GPT-5.1 only)\n",
      "\n",
      "Available reasoning_effort levels:\n",
      "   - none: No reasoning (fastest, similar to GPT-4o)\n",
      "   - minimal: Minimal reasoning\n",
      "   - low: Light reasoning\n",
      "   - medium: Moderate reasoning (default for GPT-5)\n",
      "   - high: Deep reasoning\n",
      "   - xhigh: Maximum reasoning\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# RESPONSES API - Advanced usage with verbosity control (GPT-5.1 only)\n",
    "# =============================================================================\n",
    "\n",
    "def call_gpt51_responses_api(\n",
    "    user_message: str,\n",
    "    reasoning_effort: str = \"none\",\n",
    "    verbosity: str = \"low\"\n",
    "):\n",
    "    \"\"\"\n",
    "    GPT-5.1 call using the Responses API for advanced control.\n",
    "    \n",
    "    Parameters:\n",
    "    - reasoning_effort: none, minimal, low, medium, high, xhigh\n",
    "    - verbosity: low, medium, high (new GPT-5 parameter)\n",
    "    \"\"\"\n",
    "    response = client_new.responses.create(\n",
    "        model=GPT51_DEPLOYMENT,\n",
    "        input=user_message,\n",
    "        reasoning={\n",
    "            \"effort\": reasoning_effort,\n",
    "            \"summary\": \"auto\"  # auto, concise, or detailed\n",
    "        },\n",
    "        text={\n",
    "            \"verbosity\": verbosity  # New GPT-5 parameter\n",
    "        }\n",
    "    )\n",
    "    return response\n",
    "\n",
    "print(\"Responses API function defined (GPT-5.1 only)\")\n",
    "print(\"\\nAvailable reasoning_effort levels:\")\n",
    "print(\"   - none: No reasoning (fastest, similar to GPT-4o)\")\n",
    "print(\"   - minimal: Minimal reasoning\")\n",
    "print(\"   - low: Light reasoning\")\n",
    "print(\"   - medium: Moderate reasoning (default for GPT-5)\")\n",
    "print(\"   - high: Deep reasoning\")\n",
    "print(\"   - xhigh: Maximum reasoning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Adjusting Reasoning Effort by Use Case\n",
    "\n",
    "From the [OpenAI Guide](https://platform.openai.com/docs/guides/latest-model):\n",
    "\n",
    "> *\"The `reasoning effort` parameter controls how many reasoning tokens the model generates before producing a response. Earlier reasoning models like o3 supported only low, medium, and high. With GPT-5.2, the lowest setting is none to provide lower-latency interactions.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning Effort Recommendations by Use Case:\n",
      "======================================================================\n",
      "\n",
      "Simple Q&A / Chat:\n",
      "   Effort: none\n",
      "   Fast responses, similar to GPT-4o behavior\n",
      "\n",
      "Content Generation:\n",
      "   Effort: none\n",
      "   Creative tasks don't need deep reasoning\n",
      "\n",
      "Code Review / Simple Coding:\n",
      "   Effort: low\n",
      "   Light reasoning for code analysis\n",
      "\n",
      "Complex Analysis / Reports:\n",
      "   Effort: medium\n",
      "   Balanced reasoning for analytical tasks\n",
      "\n",
      "Algorithm Design / Architecture:\n",
      "   Effort: high\n",
      "   Deep reasoning for complex technical decisions\n",
      "\n",
      "Math / Scientific Problems:\n",
      "   Effort: high\n",
      "   Maximum reasoning for precise calculations\n",
      "\n",
      "Critical Business Decisions:\n",
      "   Effort: xhigh\n",
      "   Maximum quality, cost secondary (GPT-5.2 only)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# REASONING EFFORT RECOMMENDATIONS BY USE CASE\n",
    "# =============================================================================\n",
    "\n",
    "REASONING_RECOMMENDATIONS = {\n",
    "    \"Simple Q&A / Chat\": {\n",
    "        \"effort\": \"none\",\n",
    "        \"description\": \"Fast responses, similar to GPT-4o behavior\"\n",
    "    },\n",
    "    \"Content Generation\": {\n",
    "        \"effort\": \"none\",\n",
    "        \"description\": \"Creative tasks don't need deep reasoning\"\n",
    "    },\n",
    "    \"Code Review / Simple Coding\": {\n",
    "        \"effort\": \"low\",\n",
    "        \"description\": \"Light reasoning for code analysis\"\n",
    "    },\n",
    "    \"Complex Analysis / Reports\": {\n",
    "        \"effort\": \"medium\",\n",
    "        \"description\": \"Balanced reasoning for analytical tasks\"\n",
    "    },\n",
    "    \"Algorithm Design / Architecture\": {\n",
    "        \"effort\": \"high\",\n",
    "        \"description\": \"Deep reasoning for complex technical decisions\"\n",
    "    },\n",
    "    \"Math / Scientific Problems\": {\n",
    "        \"effort\": \"high\",\n",
    "        \"description\": \"Maximum reasoning for precise calculations\"\n",
    "    },\n",
    "    \"Critical Business Decisions\": {\n",
    "        \"effort\": \"xhigh\",\n",
    "        \"description\": \"Maximum quality, cost secondary (GPT-5.2 only)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Reasoning Effort Recommendations by Use Case:\")\n",
    "print(\"=\" * 70)\n",
    "for use_case, config in REASONING_RECOMMENDATIONS.items():\n",
    "    print(f\"\\n{use_case}:\")\n",
    "    print(f\"   Effort: {config['effort']}\")\n",
    "    print(f\"   {config['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Summary: Migration Reference Card\n\n## Migration Path 1: GPT-4o -> GPT-5.1\n\n| Aspect | GPT-4o | GPT-5.1 (Phase 1) | GPT-5.1 (Phase 2) |\n|--------|--------|-------------------|-------------------|\n| **Client** | `AzureOpenAI` | `OpenAI` | `OpenAI` |\n| **Endpoint** | `.../chat/completions?api-version=...` | `.../openai/v1/chat/completions` | `.../openai/v1/responses` |\n| **Max Tokens** | `max_tokens` | `max_completion_tokens` | `max_output_tokens` |\n| **System Prompt** | `\"role\": \"system\"` | `\"role\": \"system\"` (compat) | `\"role\": \"developer\"` |\n| **Temperature** | Supported | Not supported | Not supported |\n| **Reasoning** | N/A | `reasoning_effort=\"none\"` | Adjust per use case |\n\n## Migration Path 2: GPT-4o-mini -> GPT-4.1-mini\n\n| Aspect | GPT-4o-mini | GPT-4.1-mini |\n|--------|-------------|-------------|\n| **Client** | `AzureOpenAI` | `OpenAI` |\n| **Endpoint** | `.../chat/completions?api-version=...` | `.../openai/v1/chat/completions` |\n| **Max Tokens** | `max_tokens` | `max_completion_tokens` |\n| **System Prompt** | `\"role\": \"system\"` | `\"role\": \"system\"` |\n| **Temperature** | Supported | Supported |\n| **Reasoning** | N/A | N/A (not a reasoning model) |\n\n## Official Documentation Links\n\n- **Microsoft Learn**: https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/reasoning\n- **Model Retirements**: https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/model-retirements"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Next Steps\n\n1. **Run Phase 1** with your existing prompts and evaluate results\n2. **Compare metrics** between source and target models\n3. **For GPT-5.1**: Iterate on Phase 2 optimizations based on evaluation results\n\n## Migration Decision Tree\n\n```\nWhat model are you currently using?\n+-- GPT-4o (any version)\n|   +-- Migrate to GPT-5.1\n|       +-- Remove temperature/top_p\n|       +-- Add reasoning_effort=\"none\"\n|       +-- Consider Phase 2 optimizations\n|\n+-- GPT-4o-mini\n    +-- Migrate to GPT-4.1-mini\n        +-- Keep temperature/top_p\n        +-- Just update endpoint and max_tokens\n```\n\n## Additional Resources\n\n- [Azure OpenAI Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/)\n- [Model Retirements](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/model-retirements)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}